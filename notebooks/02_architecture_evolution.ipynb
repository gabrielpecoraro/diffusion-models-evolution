{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Architecture Evolution: From UNet to MMDiT\n",
    "**How Stable Diffusion's Brain Got an Upgrade**\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook traces the architectural evolution of diffusion model denoisers from Stable Diffusion 1.5 through FLUX.1. No GPU required --- this is a theory and visualization notebook.*\n",
    "\n",
    "**What you'll learn:**\n",
    "- How the latent diffusion framework works across all SD variants\n",
    "- Why the UNet was eventually replaced by Transformers\n",
    "- What makes MMDiT (Multimodal Diffusion Transformer) a breakthrough\n",
    "- How FLUX.1 combines every major advance into a single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Latent Diffusion Framework\n",
    "\n",
    "Every Stable Diffusion variant --- from SD 1.5 to FLUX.1 --- shares the same high-level framework: **Latent Diffusion**. The key insight is that we never operate on raw pixels. Instead, we compress images into a compact **latent space** and do all the heavy lifting there.\n",
    "\n",
    "### The Three Core Components\n",
    "\n",
    "| Component | Role | Details |\n",
    "|-----------|------|---------|\n",
    "| **VAE (Encoder/Decoder)** | Compress & reconstruct images | 512x512x3 image $\\rightarrow$ 64x64x4 latent (8x spatial compression) |\n",
    "| **Denoiser (UNet or Transformer)** | Predict and remove noise from latents | The \"brain\" --- this is what evolves across SD versions |\n",
    "| **Text Encoder (CLIP, T5)** | Convert text prompts to conditioning vectors | Provides semantic guidance to the denoiser |\n",
    "\n",
    "### The Generation Pipeline\n",
    "\n",
    "```\n",
    "\"A cat wearing a top hat\"\n",
    "         |\n",
    "         v\n",
    "  [Text Encoder]  (CLIP / T5)\n",
    "         |\n",
    "         v\n",
    "  conditioning vectors\n",
    "         |\n",
    "         v\n",
    "  [Denoiser]  <--- operates on 64x64x4 latent (NOT 512x512x3 pixels!)\n",
    "   (UNet or    \n",
    "   Transformer)   Pure noise --> Structured latent (iterative denoising)\n",
    "         |\n",
    "         v\n",
    "  [VAE Decoder]  \n",
    "         |\n",
    "         v\n",
    "  512x512x3 image\n",
    "```\n",
    "\n",
    "### Why Latent Space?\n",
    "\n",
    "Working in latent space is **~64x cheaper** computationally than working in pixel space:\n",
    "\n",
    "- **Pixel space**: 512 x 512 x 3 = **786,432** values per image\n",
    "- **Latent space**: 64 x 64 x 4 = **16,384** values per latent\n",
    "- **Reduction factor**: ~48x fewer values, and the denoiser complexity scales quadratically with spatial dimensions\n",
    "\n",
    "This is what made high-resolution diffusion practical on consumer hardware.\n",
    "\n",
    "> **Key insight**: The denoiser architecture (UNet vs. Transformer) is the component that has evolved most dramatically. The VAE and the overall latent diffusion framework remain largely unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Architecture Comparison Table\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.axis('off')\n",
    "\n",
    "columns = ['Model', 'Year', 'Denoiser', 'Parameters', 'Text Encoder(s)',\n",
    "           'Training Objective', 'Typical Steps']\n",
    "\n",
    "data = [\n",
    "    ['SD 1.5',     '2022', 'UNet',  '860M',  'CLIP-L',                  'e-prediction',  '50'],\n",
    "    ['SD 2.1',     '2022', 'UNet',  '865M',  'OpenCLIP-H',             'v-prediction',  '50'],\n",
    "    ['SDXL',       '2023', 'UNet',  '2.6B',  'CLIP-L + CLIP-G',       'e-prediction',  '50'],\n",
    "    ['SD3 Medium', '2024', 'MMDiT', '2B',    'CLIP-L + CLIP-G + T5-XXL', 'Flow Matching', '28'],\n",
    "    ['FLUX.1',     '2024', 'MMDiT', '12B',   'CLIP-L + T5-XXL',       'Flow Matching', '4 (schnell)'],\n",
    "]\n",
    "\n",
    "# Row colors\n",
    "row_colors = [\n",
    "    '#E8F5E9',  # light green\n",
    "    '#E8F5E9',  # light green\n",
    "    '#FFF3E0',  # light orange\n",
    "    '#FFEBEE',  # light red\n",
    "    '#FFCDD2',  # medium red\n",
    "]\n",
    "\n",
    "header_color = '#37474F'\n",
    "header_text_color = 'white'\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=data,\n",
    "    colLabels=columns,\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.0, 1.8)\n",
    "\n",
    "# Style header row\n",
    "for j in range(len(columns)):\n",
    "    cell = table[0, j]\n",
    "    cell.set_facecolor(header_color)\n",
    "    cell.set_text_props(color=header_text_color, fontweight='bold', fontsize=10)\n",
    "    cell.set_edgecolor('white')\n",
    "    cell.set_linewidth(1.5)\n",
    "\n",
    "# Style data rows\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(columns)):\n",
    "        cell = table[i + 1, j]\n",
    "        cell.set_facecolor(row_colors[i])\n",
    "        cell.set_edgecolor('white')\n",
    "        cell.set_linewidth(1.5)\n",
    "        # Bold the model name column\n",
    "        if j == 0:\n",
    "            cell.set_text_props(fontweight='bold')\n",
    "        # Highlight denoiser column for MMDiT models\n",
    "        if j == 2 and data[i][2] == 'MMDiT':\n",
    "            cell.set_text_props(fontweight='bold', color='#C62828')\n",
    "\n",
    "ax.set_title('Diffusion Model Architecture Comparison',\n",
    "             fontsize=16, fontweight='bold', pad=20, color='#212121')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD 1.x --- The Original UNet (2022)\n",
    "\n",
    "Stable Diffusion 1.5 introduced the **UNet** as the denoiser backbone. This architecture, borrowed from medical image segmentation, turned out to be remarkably effective for iterative denoising.\n",
    "\n",
    "### UNet Architecture Overview\n",
    "\n",
    "The UNet follows an **encoder-decoder** structure with **skip connections**:\n",
    "\n",
    "1. **Encoder (Downsampling)**: Progressively reduces spatial resolution while increasing channel depth\n",
    "   - 64x64 $\\rightarrow$ 32x32 $\\rightarrow$ 16x16 $\\rightarrow$ 8x8\n",
    "   - Each level: ResNet blocks + Self-Attention + Cross-Attention\n",
    "\n",
    "2. **Bottleneck**: Processes the most compressed representation (8x8)\n",
    "\n",
    "3. **Decoder (Upsampling)**: Mirrors the encoder, progressively restoring resolution\n",
    "   - 8x8 $\\rightarrow$ 16x16 $\\rightarrow$ 32x32 $\\rightarrow$ 64x64\n",
    "   - **Skip connections** carry fine-grained spatial details from encoder to decoder\n",
    "\n",
    "### Cross-Attention: How Text Guides Generation\n",
    "\n",
    "At each resolution level, **cross-attention** layers inject text conditioning:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ (Query) comes from the **image latents**\n",
    "- $K$ (Key) and $V$ (Value) come from the **text embeddings**\n",
    "\n",
    "This is **one-way conditioning**: text informs the image, but the image cannot inform the text representation. This asymmetry becomes important when we discuss MMDiT.\n",
    "\n",
    "### Limitations of SD 1.x\n",
    "\n",
    "- **77 CLIP token limit**: Prompts are truncated beyond ~77 tokens\n",
    "- **Limited spatial understanding**: Struggles with complex compositions (\"a red cube on top of a blue sphere\")\n",
    "- **One-way conditioning**: Text cannot adapt based on what the image looks like\n",
    "- **860M parameters**: Relatively small model capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.patches as FancyBboxPatch\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# ============================================================\n",
    "# UNet Architecture Diagram\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 9))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "def draw_block(ax, x, y, w, h, label, color, fontsize=8, text_color='white'):\n",
    "    \"\"\"Draw a rounded rectangle block with a centered label.\"\"\"\n",
    "    rect = mpatches.FancyBboxPatch(\n",
    "        (x, y), w, h,\n",
    "        boxstyle=\"round,pad=0.1\",\n",
    "        facecolor=color, edgecolor='white', linewidth=1.5\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w / 2, y + h / 2, label, ha='center', va='center',\n",
    "            fontsize=fontsize, fontweight='bold', color=text_color)\n",
    "\n",
    "def draw_arrow(ax, start, end, color='#455A64', style='->'):\n",
    "    \"\"\"Draw an arrow from start to end.\"\"\"\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle=style, color=color, lw=2))\n",
    "\n",
    "# --- Title ---\n",
    "ax.text(7, 9.5, 'UNet Architecture (SD 1.5)', ha='center',\n",
    "        fontsize=16, fontweight='bold', color='#212121')\n",
    "\n",
    "# --- Encoder (left side, going down) ---\n",
    "enc_color = '#1565C0'\n",
    "enc_x = 1.5\n",
    "enc_positions = [\n",
    "    (enc_x, 7.8, 2.0, 0.9, '64x64\\nResBlock + Attn'),\n",
    "    (enc_x, 5.8, 2.0, 0.9, '32x32\\nResBlock + Attn'),\n",
    "    (enc_x, 3.8, 2.0, 0.9, '16x16\\nResBlock + Attn'),\n",
    "]\n",
    "\n",
    "for (x, y, w, h, label) in enc_positions:\n",
    "    draw_block(ax, x, y, w, h, label, enc_color, fontsize=8)\n",
    "\n",
    "# Encoder label\n",
    "ax.text(enc_x + 1.0, 9.0, 'ENCODER', ha='center', fontsize=11,\n",
    "        fontweight='bold', color=enc_color)\n",
    "\n",
    "# Encoder arrows (down)\n",
    "draw_arrow(ax, (enc_x + 1.0, 7.8), (enc_x + 1.0, 6.7), color=enc_color)\n",
    "draw_arrow(ax, (enc_x + 1.0, 5.8), (enc_x + 1.0, 4.7), color=enc_color)\n",
    "draw_arrow(ax, (enc_x + 1.0, 3.8), (enc_x + 1.0, 2.9), color=enc_color)\n",
    "\n",
    "# --- Bottleneck ---\n",
    "bneck_color = '#4A148C'\n",
    "draw_block(ax, 5.5, 1.8, 3.0, 1.0, 'BOTTLENECK\\n8x8 (most compressed)', bneck_color, fontsize=9)\n",
    "\n",
    "# Encoder to bottleneck\n",
    "draw_arrow(ax, (enc_x + 1.0, 3.8), (5.5, 2.3), color='#455A64')\n",
    "\n",
    "# --- Decoder (right side, going up) ---\n",
    "dec_color = '#C62828'\n",
    "dec_x = 10.5\n",
    "dec_positions = [\n",
    "    (dec_x, 3.8, 2.0, 0.9, '16x16\\nResBlock + Attn'),\n",
    "    (dec_x, 5.8, 2.0, 0.9, '32x32\\nResBlock + Attn'),\n",
    "    (dec_x, 7.8, 2.0, 0.9, '64x64\\nResBlock + Attn'),\n",
    "]\n",
    "\n",
    "for (x, y, w, h, label) in dec_positions:\n",
    "    draw_block(ax, x, y, w, h, label, dec_color, fontsize=8)\n",
    "\n",
    "# Decoder label\n",
    "ax.text(dec_x + 1.0, 9.0, 'DECODER', ha='center', fontsize=11,\n",
    "        fontweight='bold', color=dec_color)\n",
    "\n",
    "# Bottleneck to decoder\n",
    "draw_arrow(ax, (8.5, 2.3), (dec_x + 1.0, 3.8), color='#455A64')\n",
    "\n",
    "# Decoder arrows (up)\n",
    "draw_arrow(ax, (dec_x + 1.0, 4.7), (dec_x + 1.0, 5.8), color=dec_color)\n",
    "draw_arrow(ax, (dec_x + 1.0, 6.7), (dec_x + 1.0, 7.8), color=dec_color)\n",
    "\n",
    "# --- Skip Connections ---\n",
    "skip_color = '#FF8F00'\n",
    "for i, enc_y in enumerate([7.8, 5.8, 3.8]):\n",
    "    dec_y = [3.8, 5.8, 7.8][2 - i]\n",
    "    ax.annotate('',\n",
    "                xy=(dec_x, dec_y + 0.45), xytext=(enc_x + 2.0, enc_y + 0.45),\n",
    "                arrowprops=dict(arrowstyle='->', color=skip_color, lw=2,\n",
    "                                linestyle='dashed',\n",
    "                                connectionstyle='arc3,rad=0.0'))\n",
    "\n",
    "ax.text(7, 8.6, 'Skip Connections', ha='center', fontsize=9,\n",
    "        fontweight='bold', color=skip_color, style='italic')\n",
    "\n",
    "# --- CLIP Text Encoder (side panel) ---\n",
    "clip_color = '#00695C'\n",
    "draw_block(ax, 5.8, 5.5, 2.4, 0.7, 'CLIP Text\\nEncoder', clip_color, fontsize=9)\n",
    "\n",
    "# Cross-attention labels\n",
    "ax.text(7.0, 5.0, 'Cross-Attention\\n(text -> image)', ha='center',\n",
    "        fontsize=8, color=clip_color, style='italic')\n",
    "\n",
    "# Arrows from CLIP to encoder/decoder cross-attention points\n",
    "# Left arrows (to encoder blocks)\n",
    "draw_arrow(ax, (5.8, 5.85), (3.5, 6.25), color=clip_color)\n",
    "draw_arrow(ax, (5.8, 5.85), (3.5, 4.25), color=clip_color)\n",
    "\n",
    "# Right arrows (to decoder blocks)\n",
    "draw_arrow(ax, (8.2, 5.85), (10.5, 6.25), color=clip_color)\n",
    "draw_arrow(ax, (8.2, 5.85), (10.5, 4.25), color=clip_color)\n",
    "\n",
    "# --- Input / Output labels ---\n",
    "ax.text(enc_x + 1.0, 9.0 - 0.3, '(downsampling)', ha='center',\n",
    "        fontsize=8, color=enc_color, style='italic')\n",
    "ax.text(dec_x + 1.0, 9.0 - 0.3, '(upsampling)', ha='center',\n",
    "        fontsize=8, color=dec_color, style='italic')\n",
    "\n",
    "# --- Legend ---\n",
    "legend_items = [\n",
    "    mpatches.Patch(color=enc_color, label='Encoder blocks (downsample)'),\n",
    "    mpatches.Patch(color=dec_color, label='Decoder blocks (upsample)'),\n",
    "    mpatches.Patch(color=bneck_color, label='Bottleneck'),\n",
    "    mpatches.Patch(color=clip_color, label='Text conditioning (cross-attention)'),\n",
    "    mpatches.Patch(color=skip_color, label='Skip connections'),\n",
    "]\n",
    "ax.legend(handles=legend_items, loc='lower center', ncol=3,\n",
    "          fontsize=8, framealpha=0.9, edgecolor='#BDBDBD')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDXL --- Scaling the UNet (2023)\n",
    "\n",
    "SDXL represented the peak of UNet-based diffusion models, demonstrating that significant quality gains were still possible within the existing architecture.\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "| Feature | SD 1.5 | SDXL |\n",
    "|---------|--------|------|\n",
    "| Parameters | 860M | **2.6B** (3x larger) |\n",
    "| Text Encoders | CLIP-L only | **CLIP-L + CLIP-G** (dual encoder) |\n",
    "| Base Resolution | 512x512 | **1024x1024** |\n",
    "| Conditioning | Text only | Text + **micro-conditioning** |\n",
    "\n",
    "### Dual CLIP Encoders\n",
    "\n",
    "SDXL concatenates embeddings from two different CLIP models:\n",
    "- **CLIP-L** (ViT-L/14): Good at understanding concepts and objects\n",
    "- **CLIP-G** (ViT-bigG/14): Better at understanding style, composition, and nuance\n",
    "\n",
    "The concatenated embedding provides a richer, more nuanced text representation.\n",
    "\n",
    "### Micro-Conditioning\n",
    "\n",
    "SDXL introduced conditioning on **image metadata** during training:\n",
    "- **Original resolution** of the training image\n",
    "- **Crop coordinates** (top, left)\n",
    "- **Target resolution** for generation\n",
    "\n",
    "This eliminated common artifacts like awkward cropping and helped the model understand image composition better.\n",
    "\n",
    "### Optional Refiner Model\n",
    "\n",
    "SDXL can optionally use a **refiner** model that takes the base model's output and enhances fine details. This two-stage approach improves texture quality and small details at the cost of additional inference time.\n",
    "\n",
    "> **SDXL proved that UNets could still scale, but also revealed their limits** --- the architecture was becoming increasingly complex and difficult to scale further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SD3 --- The MMDiT Revolution (2024)\n",
    "\n",
    "Stable Diffusion 3 represents the most significant architectural shift in the SD lineage: **replacing the UNet entirely with a Transformer**. This is the KEY innovation that separates the \"old\" and \"new\" eras of diffusion models.\n",
    "\n",
    "---\n",
    "\n",
    "### From UNet to Transformer: Why?\n",
    "\n",
    "The DiT paper (Peebles & Xie, 2023) demonstrated that Vision Transformers could replace UNets as diffusion denoisers. The advantages:\n",
    "\n",
    "1. **Better scaling**: Transformers scale more predictably with parameters (well-studied scaling laws)\n",
    "2. **Simpler architecture**: No encoder/decoder asymmetry, no skip connections\n",
    "3. **Unified attention**: All information flows through the same attention mechanism\n",
    "4. **Hardware efficiency**: Transformers are better optimized on modern GPUs/TPUs\n",
    "\n",
    "---\n",
    "\n",
    "### MMDiT: Multimodal Diffusion Transformer\n",
    "\n",
    "SD3's specific innovation is the **MMDiT (Multimodal Diffusion Transformer)** block. This is not just \"a Transformer for images\" --- it fundamentally changes how text and image interact.\n",
    "\n",
    "#### The Core Idea: Two Streams, Joint Attention\n",
    "\n",
    "MMDiT maintains **two separate processing streams**:\n",
    "\n",
    "| Stream | Input | Purpose |\n",
    "|--------|-------|---------|\n",
    "| **Image stream** | Noisy image latents (patchified) | Processes visual information |\n",
    "| **Text stream** | Text encoder outputs | Processes linguistic information |\n",
    "\n",
    "Each stream has its own:\n",
    "- LayerNorm\n",
    "- QKV (Query, Key, Value) projections\n",
    "- Feed-Forward Network (MLP)\n",
    "\n",
    "But they **share a single attention operation** --- this is the critical difference.\n",
    "\n",
    "#### Joint Attention: The Breakthrough\n",
    "\n",
    "In the UNet's cross-attention:\n",
    "```\n",
    "Q = image,  K = text,  V = text     (text -> image, ONE-WAY)\n",
    "```\n",
    "\n",
    "In MMDiT's joint attention:\n",
    "```\n",
    "Q = [image_Q ; text_Q]              (concatenated)\n",
    "K = [image_K ; text_K]              (concatenated)  \n",
    "V = [image_V ; text_V]              (concatenated)\n",
    "\n",
    "Attention = softmax(QK^T / sqrt(d)) * V    (TWO-WAY!)\n",
    "```\n",
    "\n",
    "This means:\n",
    "- **Image tokens attend to text tokens** (as before)\n",
    "- **Text tokens attend to image tokens** (NEW!)\n",
    "- **Image tokens attend to other image tokens** (self-attention)\n",
    "- **Text tokens attend to other text tokens** (self-attention)\n",
    "\n",
    "All four types of attention happen in a **single attention matrix**.\n",
    "\n",
    "#### Why Two-Way Attention Matters\n",
    "\n",
    "With one-way cross-attention (UNet), the text representation is **frozen** --- it cannot adapt based on the current state of the image. With joint attention (MMDiT), the text representation is **dynamically refined** at every layer based on the evolving image.\n",
    "\n",
    "This enables:\n",
    "- Better spatial reasoning (\"the cat is ON TOP OF the box\")\n",
    "- More accurate multi-object compositions\n",
    "- Better text rendering in images\n",
    "- More faithful prompt following overall\n",
    "\n",
    "---\n",
    "\n",
    "### Triple Text Encoders\n",
    "\n",
    "SD3 uses **three** text encoders simultaneously:\n",
    "\n",
    "| Encoder | Type | Token Limit | Strength |\n",
    "|---------|------|-------------|----------|\n",
    "| CLIP-L | Contrastive | 77 | Object/concept understanding |\n",
    "| CLIP-G | Contrastive | 77 | Style/composition understanding |\n",
    "| T5-XXL | Generative (encoder-only) | 512 | Long, detailed text understanding |\n",
    "\n",
    "The T5-XXL encoder is particularly important: it breaks the 77-token barrier, enabling detailed prompts that describe complex scenes.\n",
    "\n",
    "---\n",
    "\n",
    "### Flow Matching Replaces DDPM\n",
    "\n",
    "SD3 also replaces the traditional DDPM noise schedule with **Rectified Flow Matching** (covered in depth in the next notebook). The key benefit: straighter denoising trajectories that require **fewer sampling steps**.\n",
    "\n",
    "> **SD3 is where everything changed.** MMDiT + Flow Matching + triple encoders = a completely new generation of diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# ============================================================\n",
    "# MMDiT Block Diagram\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 11)\n",
    "ax.axis('off')\n",
    "\n",
    "def draw_box(ax, x, y, w, h, label, color, fontsize=9, text_color='white',\n",
    "             alpha=1.0, linestyle='-'):\n",
    "    \"\"\"Draw a rounded box with label.\"\"\"\n",
    "    rect = mpatches.FancyBboxPatch(\n",
    "        (x, y), w, h,\n",
    "        boxstyle=\"round,pad=0.12\",\n",
    "        facecolor=color, edgecolor='white', linewidth=1.5,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w / 2, y + h / 2, label, ha='center', va='center',\n",
    "            fontsize=fontsize, fontweight='bold', color=text_color)\n",
    "\n",
    "def arrow(ax, start, end, color='#455A64'):\n",
    "    ax.annotate('', xy=end, xytext=start,\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=2))\n",
    "\n",
    "# --- Title ---\n",
    "ax.text(7, 10.5, 'MMDiT Block (Multimodal Diffusion Transformer)',\n",
    "        ha='center', fontsize=16, fontweight='bold', color='#212121')\n",
    "ax.text(7, 10.0, 'Two-way information flow between image and text',\n",
    "        ha='center', fontsize=11, color='#616161', style='italic')\n",
    "\n",
    "# --- Colors ---\n",
    "img_color = '#1565C0'      # blue for image stream\n",
    "txt_color = '#C62828'      # red for text stream\n",
    "joint_color = '#6A1B9A'    # purple for joint attention\n",
    "ffn_img = '#1976D2'        # lighter blue\n",
    "ffn_txt = '#E53935'        # lighter red\n",
    "norm_color = '#546E7A'     # gray for norms\n",
    "\n",
    "# ===================== LEFT STREAM (Image) =====================\n",
    "left_x = 1.5\n",
    "stream_w = 3.0\n",
    "\n",
    "# Input\n",
    "draw_box(ax, left_x, 8.5, stream_w, 0.7, 'Image Latents (patchified)',\n",
    "         img_color, fontsize=9)\n",
    "\n",
    "# LayerNorm\n",
    "draw_box(ax, left_x, 7.3, stream_w, 0.6, 'LayerNorm', norm_color, fontsize=9)\n",
    "arrow(ax, (left_x + stream_w / 2, 8.5), (left_x + stream_w / 2, 7.9))\n",
    "\n",
    "# QKV Projection\n",
    "draw_box(ax, left_x, 6.1, stream_w, 0.7, 'Q_img, K_img, V_img\\n(Linear Projections)',\n",
    "         img_color, fontsize=8)\n",
    "arrow(ax, (left_x + stream_w / 2, 7.3), (left_x + stream_w / 2, 6.8))\n",
    "\n",
    "# ===================== RIGHT STREAM (Text) =====================\n",
    "right_x = 9.5\n",
    "\n",
    "# Input\n",
    "draw_box(ax, right_x, 8.5, stream_w, 0.7, 'Text Tokens (from CLIP/T5)',\n",
    "         txt_color, fontsize=9)\n",
    "\n",
    "# LayerNorm\n",
    "draw_box(ax, right_x, 7.3, stream_w, 0.6, 'LayerNorm', norm_color, fontsize=9)\n",
    "arrow(ax, (right_x + stream_w / 2, 8.5), (right_x + stream_w / 2, 7.9))\n",
    "\n",
    "# QKV Projection\n",
    "draw_box(ax, right_x, 6.1, stream_w, 0.7, 'Q_txt, K_txt, V_txt\\n(Linear Projections)',\n",
    "         txt_color, fontsize=8)\n",
    "arrow(ax, (right_x + stream_w / 2, 7.3), (right_x + stream_w / 2, 6.8))\n",
    "\n",
    "# ===================== JOINT ATTENTION (Center) =====================\n",
    "joint_x = 4.0\n",
    "joint_w = 6.0\n",
    "\n",
    "draw_box(ax, joint_x, 4.3, joint_w, 1.2,\n",
    "         'JOINT ATTENTION\\n'\n",
    "         'Q = [Q_img ; Q_txt]   K = [K_img ; K_txt]   V = [V_img ; V_txt]',\n",
    "         joint_color, fontsize=9)\n",
    "\n",
    "# Arrows from both QKV blocks into joint attention\n",
    "arrow(ax, (left_x + stream_w / 2, 6.1), (joint_x + 1.5, 5.5), color=img_color)\n",
    "arrow(ax, (right_x + stream_w / 2, 6.1), (joint_x + joint_w - 1.5, 5.5),\n",
    "      color=txt_color)\n",
    "\n",
    "# Label the two-way flow\n",
    "ax.text(7, 3.8, 'Image attends to Text  &  Text attends to Image',\n",
    "        ha='center', fontsize=10, fontweight='bold', color=joint_color,\n",
    "        style='italic')\n",
    "\n",
    "# ===================== OUTPUTS: Split back =====================\n",
    "# FFN Image\n",
    "draw_box(ax, left_x, 2.2, stream_w, 0.7, 'Feed-Forward Net\\n(Image)', ffn_img, fontsize=9)\n",
    "arrow(ax, (joint_x + 1.5, 4.3), (left_x + stream_w / 2, 2.9), color=img_color)\n",
    "\n",
    "# FFN Text\n",
    "draw_box(ax, right_x, 2.2, stream_w, 0.7, 'Feed-Forward Net\\n(Text)', ffn_txt, fontsize=9)\n",
    "arrow(ax, (joint_x + joint_w - 1.5, 4.3), (right_x + stream_w / 2, 2.9),\n",
    "      color=txt_color)\n",
    "\n",
    "# Output labels\n",
    "draw_box(ax, left_x, 1.0, stream_w, 0.6, 'Updated Image Latents',\n",
    "         img_color, fontsize=9, alpha=0.7)\n",
    "arrow(ax, (left_x + stream_w / 2, 2.2), (left_x + stream_w / 2, 1.6),\n",
    "      color=img_color)\n",
    "\n",
    "draw_box(ax, right_x, 1.0, stream_w, 0.6, 'Updated Text Tokens',\n",
    "         txt_color, fontsize=9, alpha=0.7)\n",
    "arrow(ax, (right_x + stream_w / 2, 2.2), (right_x + stream_w / 2, 1.6),\n",
    "      color=txt_color)\n",
    "\n",
    "# --- Comparison annotation ---\n",
    "ax.text(7, 0.3,\n",
    "        'UNet cross-attention: text \\u2192 image (one-way)  |  '\n",
    "        'MMDiT joint attention: text \\u2194 image (two-way)',\n",
    "        ha='center', fontsize=10, fontweight='bold',\n",
    "        color='#37474F',\n",
    "        bbox=dict(boxstyle='round,pad=0.4', facecolor='#FFF9C4',\n",
    "                  edgecolor='#F9A825', linewidth=1.5))\n",
    "\n",
    "# --- Stream labels ---\n",
    "ax.text(left_x + stream_w / 2, 9.5, 'IMAGE STREAM',\n",
    "        ha='center', fontsize=12, fontweight='bold', color=img_color)\n",
    "ax.text(right_x + stream_w / 2, 9.5, 'TEXT STREAM',\n",
    "        ha='center', fontsize=12, fontweight='bold', color=txt_color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FLUX.1 --- Pushing the Frontier (August 2024)\n",
    "\n",
    "FLUX.1, released by **Black Forest Labs** in August 2024, represents the state of the art in open-source image generation. The team behind it includes the original creators of Stable Diffusion who left Stability AI to found their own company.\n",
    "\n",
    "### What Makes FLUX.1 Special\n",
    "\n",
    "| Feature | Details |\n",
    "|---------|--------|\n",
    "| **Scale** | 12 billion parameters --- 6x larger than SD3 Medium |\n",
    "| **Architecture** | MMDiT (inherited from SD3) |\n",
    "| **Text Encoders** | CLIP-L + T5-XXL (dual encoder) |\n",
    "| **Training** | Flow Matching + Guidance Distillation |\n",
    "| **License** | Apache 2.0 (schnell) --- fully open source |\n",
    "\n",
    "### Three Variants\n",
    "\n",
    "| Variant | Steps | Speed | Use Case | License |\n",
    "|---------|-------|-------|----------|--------|\n",
    "| **FLUX.1-schnell** | 4 | Very fast | Real-time / interactive | Apache 2.0 |\n",
    "| **FLUX.1-dev** | ~50 | Moderate | High-quality generation | Non-commercial |\n",
    "| **FLUX.1-pro** | ~50 | Moderate | Commercial applications | Commercial API |\n",
    "\n",
    "### Guidance Distillation: The Speed Secret\n",
    "\n",
    "Traditional diffusion models use **classifier-free guidance (CFG)** at inference time, which requires **two forward passes** per step:\n",
    "1. One pass with the text prompt (conditional)\n",
    "2. One pass without text (unconditional)\n",
    "3. Final output = unconditional + scale * (conditional - unconditional)\n",
    "\n",
    "**Guidance distillation** trains the model to internalize this guidance behavior, so it only needs a **single forward pass** per step. Combined with flow matching's straighter trajectories, this enables FLUX.1-schnell to generate quality images in just **4 steps**.\n",
    "\n",
    "### The Significance of FLUX.1\n",
    "\n",
    "FLUX.1 is significant not just for its quality, but for what it proves:\n",
    "- **MMDiT scales**: Going from 2B (SD3) to 12B yields clear quality improvements\n",
    "- **Open source can compete**: FLUX.1-schnell matches or exceeds many closed models\n",
    "- **Speed and quality are not trade-offs**: With the right training (guidance distillation + flow matching), you can have both\n",
    "- **The SD lineage continues**: Despite the architectural revolution, the latent diffusion framework remains the foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Parameter Scaling Visualization\n",
    "# ============================================================\n",
    "\n",
    "models = ['SD 1.5\\n(2022)', 'SD 2.1\\n(2022)', 'SDXL\\n(2023)', 'SD3 Med\\n(2024)', 'FLUX.1\\n(2024)']\n",
    "params = [0.86, 0.865, 2.6, 2.0, 12.0]\n",
    "colors = ['#4CAF50', '#66BB6A', '#FFA726', '#FF7043', '#E53935']\n",
    "denoiser = ['UNet', 'UNet', 'UNet', 'MMDiT', 'MMDiT']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(models, params, color=colors, edgecolor='white', linewidth=2)\n",
    "\n",
    "for bar, param, arch in zip(bars, params, denoiser):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.3,\n",
    "            f'{param}B\\n({arch})', ha='center', va='bottom',\n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Parameters (Billions)', fontsize=12)\n",
    "ax.set_title('Diffusion Model Parameter Scaling: 2022-2024',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 15)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add annotation for architecture shift\n",
    "ax.axvline(x=2.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(1.0, 14, 'UNet Era', ha='center', fontsize=12, style='italic', color='gray')\n",
    "ax.text(3.5, 14, 'Transformer Era', ha='center', fontsize=12, style='italic', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# Key Innovations Timeline\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.set_xlim(2019.5, 2025)\n",
    "ax.set_ylim(-2, 5)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(2022.25, 4.7, 'Timeline of Diffusion Model Innovations',\n",
    "        ha='center', fontsize=16, fontweight='bold', color='#212121')\n",
    "\n",
    "# Main timeline axis\n",
    "ax.plot([2019.8, 2024.9], [0, 0], color='#37474F', linewidth=3, zorder=1)\n",
    "\n",
    "# Year markers\n",
    "for year in [2020, 2021, 2022, 2023, 2024]:\n",
    "    ax.plot(year, 0, 'o', color='#37474F', markersize=10, zorder=2)\n",
    "    ax.text(year, -0.5, str(year), ha='center', fontsize=11,\n",
    "            fontweight='bold', color='#37474F')\n",
    "\n",
    "# Events\n",
    "events = [\n",
    "    (2020, 1.5, 'DDPM\\n(Ho et al.)', '#4CAF50'),\n",
    "    (2021, 2.5, 'Improved DDPM\\n+\\nLatent Diffusion\\n(Rombach et al.)', '#66BB6A'),\n",
    "    (2022, 3.5, 'SD 1.x / SD 2.x\\n(Public Release)', '#2196F3'),\n",
    "    (2023, 2.5, 'SDXL (2.6B UNet)\\n+\\nDiT Paper\\n(Peebles & Xie)', '#FFA726'),\n",
    "    (2024.15, 3.5, 'SD3 (Feb)\\nMMDiT + Flow\\nMatching', '#FF7043'),\n",
    "    (2024.6, 1.5, 'FLUX.1 (Aug)\\n12B MMDiT\\n+ Guidance\\nDistillation', '#E53935'),\n",
    "]\n",
    "\n",
    "for (x, y, label, color) in events:\n",
    "    # Vertical connector line\n",
    "    ax.plot([x, x], [0.15, y - 0.15], color=color, linewidth=2, zorder=1)\n",
    "    # Event dot on timeline\n",
    "    ax.plot(x, 0, 'o', color=color, markersize=14, zorder=3)\n",
    "    # Label box\n",
    "    ax.text(x, y, label, ha='center', va='bottom', fontsize=8.5,\n",
    "            fontweight='bold', color='white',\n",
    "            bbox=dict(boxstyle='round,pad=0.4', facecolor=color,\n",
    "                      edgecolor='white', linewidth=1.5, alpha=0.95))\n",
    "\n",
    "# Architecture era annotation\n",
    "ax.annotate('', xy=(2024.0, -1.2), xytext=(2020.0, -1.2),\n",
    "            arrowprops=dict(arrowstyle='<->', color='#1565C0', lw=2))\n",
    "ax.text(2022.0, -1.5, 'UNet-based denoiser', ha='center', fontsize=10,\n",
    "        color='#1565C0', fontweight='bold')\n",
    "\n",
    "ax.annotate('', xy=(2024.9, -1.2), xytext=(2024.0, -1.2),\n",
    "            arrowprops=dict(arrowstyle='<->', color='#C62828', lw=2))\n",
    "ax.text(2024.45, -1.5, 'Transformer\\n(MMDiT)', ha='center', fontsize=10,\n",
    "        color='#C62828', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "---\n",
    "\n",
    "### 1. UNet to Transformer: A Paradigm Shift\n",
    "The shift from UNet to Transformer-based denoisers (DiT/MMDiT) enables better scaling, simpler architectures, and more effective use of compute. Transformers follow well-understood scaling laws --- more parameters reliably means better quality.\n",
    "\n",
    "### 2. MMDiT's Two-Way Attention Is the Critical Innovation\n",
    "The UNet's cross-attention only allowed text to influence image generation (one-way). MMDiT's joint attention enables **bidirectional information flow** --- text and image representations refine each other at every layer. This is why SD3 and FLUX.1 are dramatically better at prompt following.\n",
    "\n",
    "### 3. Flow Matching Enables Fewer Steps\n",
    "By training with rectified flow matching instead of DDPM, the denoising trajectories become straighter and require fewer steps to traverse. SD3 needs ~28 steps vs. SD 1.5's 50 steps for comparable quality.\n",
    "\n",
    "### 4. Guidance Distillation Enables Real-Time Generation\n",
    "FLUX.1-schnell combines flow matching with guidance distillation to generate quality images in just 4 steps with a single forward pass per step. This makes real-time diffusion generation practical.\n",
    "\n",
    "### 5. The Latent Diffusion Framework Endures\n",
    "Despite all the architectural changes in the denoiser, the overall framework remains the same: encode to latent space, denoise, decode. This stability means many tools and techniques (LoRA, ControlNet, IP-Adapter) can be adapted across model generations.\n",
    "\n",
    "---\n",
    "\n",
    "| Era | Models | Key Advance |\n",
    "|-----|--------|-------------|\n",
    "| **UNet Era** | SD 1.5, SD 2.1, SDXL | Cross-attention conditioning, scaling parameters |\n",
    "| **Transformer Era** | SD3, FLUX.1 | MMDiT joint attention, flow matching, guidance distillation |\n",
    "\n",
    "---\n",
    "\n",
    "*Next notebook: We dive deep into Flow Matching --- the training paradigm that replaced DDPM and enabled the efficiency gains of SD3 and FLUX.1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. **Rombach et al.** (2022). *High-Resolution Image Synthesis with Latent Diffusion Models.* CVPR 2022. [arXiv:2112.10752](https://arxiv.org/abs/2112.10752)\n",
    "\n",
    "2. **Peebles & Xie** (2023). *Scalable Diffusion Models with Transformers (DiT).* ICCV 2023. [arXiv:2212.09748](https://arxiv.org/abs/2212.09748)\n",
    "\n",
    "3. **Esser et al.** (2024). *Scaling Rectified Flow Transformers for High-Resolution Image Synthesis (SD3).* [arXiv:2403.03206](https://arxiv.org/abs/2403.03206)\n",
    "\n",
    "4. **Black Forest Labs** (2024). *FLUX.1: An open-source text-to-image model.* [https://blackforestlabs.ai](https://blackforestlabs.ai)\n",
    "\n",
    "5. **Ho et al.** (2020). *Denoising Diffusion Probabilistic Models.* NeurIPS 2020. [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)\n",
    "\n",
    "6. **Podell et al.** (2023). *SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis.* [arXiv:2307.01952](https://arxiv.org/abs/2307.01952)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}