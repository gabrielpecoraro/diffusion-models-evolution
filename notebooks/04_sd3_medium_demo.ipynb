{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stable Diffusion 3 Medium \u2014 Demo on Apple Silicon\n",
    "**Running the MMDiT Revolution on 16GB**\n",
    "\n",
    "SD3 Medium introduced the Multi-Modal Diffusion Transformer (MMDiT) architecture, replacing the traditional UNet\n",
    "with a transformer that jointly processes text and image tokens through two-way attention. This notebook\n",
    "demonstrates real inference on a memory-constrained 16GB Apple Silicon Mac using strategic optimizations:\n",
    "dropping the T5-XXL text encoder, using float16 precision, and enabling CPU offloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure the following:\n",
    "\n",
    "1. **HuggingFace account + token** \u2014 Run `huggingface-cli login` in your terminal and paste your access token.\n",
    "2. **Accept the SD3 Medium license** \u2014 Visit [stabilityai/stable-diffusion-3-medium-diffusers](https://huggingface.co/stabilityai/stable-diffusion-3-medium-diffusers) and accept the model license agreement.\n",
    "3. **16GB memory strategy** \u2014 SD3 Medium ships with three text encoders (CLIP-L, CLIP-G, T5-XXL). T5-XXL alone requires ~14GB in float16, making it impossible to fit the full pipeline in 16GB. Our strategy:\n",
    "   - **Drop T5-XXL** (`drop_t5_encoder=True`): CLIP-L + CLIP-G still provide excellent text understanding.\n",
    "   - **Use float16**: Halves memory compared to float32 with negligible quality loss.\n",
    "   - **CPU offloading**: Moves inactive pipeline components to CPU RAM during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), \"\"))\n",
    "# If running from notebooks dir:\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from config.default import DiffusionConfig\n",
    "from models.memory_utils import setup_mps_environment, clear_memory, log_memory_usage\n",
    "from models.pipeline_factory import load_pipeline, generate_image\n",
    "from models.prompt_bank import BENCHMARK_PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DiffusionConfig(\n",
    "    model_name=\"sd3-medium\",\n",
    "    quantization=\"none\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.0,\n",
    "    height=512,\n",
    "    width=512,\n",
    "    seed=42,\n",
    "    dtype=\"float16\",\n",
    "    enable_cpu_offload=True,\n",
    "    drop_t5_encoder=True,\n",
    ")\n",
    "\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Steps: {config.num_inference_steps}\")\n",
    "print(f\"Resolution: {config.width}x{config.height}\")\n",
    "print(f\"T5-XXL dropped: {config.drop_t5_encoder} (saves ~14GB)\")\n",
    "print(f\"CPU offload: {config.enable_cpu_offload}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Baseline\n",
    "\n",
    "We measure process memory (RSS) at three key points to understand the memory profile:\n",
    "1. **Before loading** \u2014 baseline with just Python and imports.\n",
    "2. **After loading** \u2014 the pipeline is in memory (with T5-XXL dropped and CPU offloading active).\n",
    "3. **After generation** \u2014 peak memory during inference when model components move to the MPS device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_memory_usage(\"before loading\")\n",
    "print(\"Loading SD3 Medium (this may take a few minutes on first run)...\")\n",
    "print(\"Note: T5-XXL encoder is dropped to fit in 16GB\")\n",
    "\n",
    "pipe = load_pipeline(config)\n",
    "\n",
    "log_memory_usage(\"after loading SD3 Medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Generation\n",
    "\n",
    "Let's generate our first image with the MMDiT architecture. We use a classic prompt that tests\n",
    "photorealism, spatial composition, and cinematic lighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A photorealistic astronaut riding a white horse on Mars, cinematic lighting\"\n",
    "config.prompt = prompt\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generating with {config.num_inference_steps} steps...\")\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "image = generate_image(pipe, config)\n",
    "elapsed = time.perf_counter() - start_time\n",
    "\n",
    "print(f\"Generation time: {elapsed:.1f}s\")\n",
    "log_memory_usage(\"after generation\")\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.imshow(image)\n",
    "ax.set_title(f\"SD3 Medium | {config.num_inference_steps} steps | {elapsed:.1f}s\", fontsize=13, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "os.makedirs(os.path.join(\"..\", \"outputs\", \"sd3_medium\"), exist_ok=True)\n",
    "image.save(os.path.join(\"..\", \"outputs\", \"sd3_medium\", \"astronaut.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Prompt Suite\n",
    "\n",
    "Now we run all 5 benchmark prompts to test different capabilities that improved with the MMDiT\n",
    "architecture: text rendering, photorealism, spatial reasoning, artistic style, and complex scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "times = []\n",
    "\n",
    "for i, ps in enumerate(BENCHMARK_PROMPTS):\n",
    "    print(f\"[{i+1}/{len(BENCHMARK_PROMPTS)}] {ps.category}: {ps.prompt[:60]}...\")\n",
    "    config.prompt = ps.prompt\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    img = generate_image(pipe, config)\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    images.append(img)\n",
    "    times.append(elapsed)\n",
    "    print(f\"  Done in {elapsed:.1f}s\")\n",
    "\n",
    "# Display grid\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(20, 4))\n",
    "for ax, img, ps, t in zip(axes, images, BENCHMARK_PROMPTS, times):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"{ps.category}\\n{t:.1f}s\", fontsize=10, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "fig.suptitle(\"SD3 Medium \\u2014 Benchmark Prompt Suite (512x512)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save all\n",
    "for img, ps in zip(images, BENCHMARK_PROMPTS):\n",
    "    img.save(os.path.join(\"..\", \"outputs\", \"sd3_medium\", f\"{ps.name}.png\"))\n",
    "print(f\"Saved {len(images)} images to outputs/sd3_medium/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary\n",
    "\n",
    "Summary of inference performance for SD3 Medium on Apple Silicon with the 16GB-optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SD3 Medium Performance Summary (16GB Apple Silicon)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Prompt':<20} {'Category':<18} {'Time (s)':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for ps, t in zip(BENCHMARK_PROMPTS, times):\n",
    "    print(f\"{ps.name:<20} {ps.category:<18} {t:<10.1f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Average':<20} {'':<18} {sum(times)/len(times):<10.1f}\")\n",
    "print(f\"\\nResolution: {config.width}x{config.height}\")\n",
    "print(f\"Steps: {config.num_inference_steps}\")\n",
    "print(f\"Guidance Scale: {config.guidance_scale}\")\n",
    "\n",
    "log_memory_usage(\"final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe\n",
    "clear_memory()\n",
    "log_memory_usage(\"after cleanup\")\n",
    "print(\"Pipeline unloaded. Memory cleared for next model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Key observations from the SD3 Medium demo:\n",
    "\n",
    "- **Text rendering**: MMDiT's two-way attention significantly improves text in images compared to SDXL. The dual CLIP encoders (even without T5-XXL) give the model much stronger text comprehension.\n",
    "- **Spatial reasoning**: Better understanding of spatial relationships between objects (e.g., \"on top of\", \"behind\") thanks to the joint text-image token processing.\n",
    "- **Memory**: Dropping T5-XXL is essential for 16GB \u2014 quality is still excellent with the dual CLIP encoders (CLIP-L + CLIP-G). The full pipeline with T5-XXL would require ~30GB.\n",
    "- **Speed**: 28 steps is the sweet spot for quality vs speed on the MMDiT scheduler. Fewer steps degrade quality noticeably; more steps offer diminishing returns.\n",
    "\n",
    "**Next**: See how FLUX.1-schnell compares in notebook 05 \u2014 it uses a distilled flow matching approach that needs only 4 steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 2,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}